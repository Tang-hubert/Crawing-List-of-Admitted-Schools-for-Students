# Crawing-List-of-Admitted-Schools-for-Students

## Get Data

This project is designed to crawl and process lists of admitted students for various departments from a specific website. Data is initially obtained by manually saving HTML pages from the department list website and then processed using Python scripts to extract and organize the information.

### Website for Data Collection

*   [Department List](https://www.com.tw/cross/university_030_112.html) - Use Ctrl+S to save each department's page locally.

## How to Run the Code

> **Note:** The code has been refactored into Python scripts for better organization and execution.

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd Crawing-List-of-Admitted-Schools-for-Students
    ```

2.  **Collect HTML Data:**
    *   Visit the [department list website](https://www.com.tw/cross/university_030_112.html).
    *   For each department's page you want to process, use **Ctrl+S** (or Cmd+S on macOS) to save the HTML file.
    *   **Important:** Save these HTML files into the `./data/department` folder. Ensure the filenames are descriptive (the default filename when saving should be sufficient).

    > **Alternative Data Collection (Selenium - Optional):**
    > If you prefer automated data collection, you can explore using a library like [Selenium](https://medium.com/marketingdatascience/selenium%E6%95%99%E5%AD%B8-%E4%B8%80-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8webdriver-send-keys-988816ce9bed) to automate the process of downloading HTML files. However, this project currently relies on manually saved HTML files.

3.  **Install Libraries:**
    Ensure you have the necessary Python libraries installed. It is recommended to create a virtual environment first.
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Linux/macOS
    venv\Scripts\activate  # On Windows
    pip install -r requirements.txt
    ```
    > **Tesseract OCR:** You also need to have [Tesseract OCR](https://tesseract-ocr.github.io/) installed on your system. Make sure to add the Tesseract executable directory to your system's PATH environment variable, or you will need to specify the path in the Python script.

4.  **Run `get_location_url.py`:**
    This script processes the HTML files in the `./data/department` folder to extract university and department location URLs.
    ```bash
    python get_location_url.py
    ```
    *   **Configuration:**
        *   Modify `input_directory` and `result_directory` variables in `get_location_url.py` if your input and output directories are different.

    *   **Output:**
        *   University-specific CSV files will be created in the `./result/location` folder (e.g., `長庚大學.csv`).
        *   A combined `whole.csv` file containing data from all processed universities will also be created in the `./result/location` folder.

5.  **Run `get_side_rank.py`:**
    This script processes the same HTML files again to extract rank information using OCR (Optical Character Recognition) and combines it with other data.
    ```bash
    python get_side_rank.py
    ```
    *   **Configuration:**
        *   Modify `input_directory` and `result_directory` variables in `get_side_rank.py` if your input and output directories are different.
        *   **Crucially:** Update `tesseract_exe_path` in `get_side_rank.py` to the correct path of your Tesseract OCR executable.  For example: `tesseract_exe_path = r'C:\Program Files\Tesseract-OCR\tesseract.exe'`

    *   **Output:**
        *   University-specific CSV files with rank information will be created in the `./result` folder (e.g., `長庚大學.csv`). These files will overwrite any previous CSV files with the same name in this folder if they exist.

6.  **(Optional) Combine Location and Rank Data:**
    If you want to merge the location URLs from `whole.csv` (generated by `get_location_url.py`) with the rank data from the CSVs generated by `get_side_rank.py`, you would need to write an additional script or use a tool like Pandas to join the dataframes based on common keys (e.g., university and department name). Currently, this merging step is not automated in the provided scripts.

7.  **Verify and Correct Data:**
    *   **Rank and Department Rank Accuracy:** The OCR process for extracting `rank` and `dept_rank` might not be 100% accurate (expect approximately 60% accuracy).
    *   **Manual Verification:**  Manually check the `dept_rank` column in the output CSV files and correct any OCR errors. You may need to compare the extracted ranks against the original HTML pages.

8.  **Final Result:**
    After running these scripts and performing manual verification, you will have CSV files in the `./result` folder containing lists of admitted students with extracted rank information for each university and department.

## Requirements

*   Python 3.8 or higher
*   **Python Libraries (install using `pip install -r requirements.txt`):**
    *   `beautifulsoup4`
    *   `pandas`
    *   `pytesseract`
    *   `Pillow` (PIL)
    *   `opencv-python`
*   **Tesseract OCR Engine:**
    *   Install Tesseract OCR from [https://tesseract-ocr.github.io/](https://tesseract-ocr.github.io/)
    *   Ensure the Tesseract executable is in your system's PATH or correctly specified in `get_side_rank.py`.

## TO-DO

*   **(Efficiency Improvement - Not Sure):**  Explore efficiency improvements by calculating and storing student-specific search results (for each `exam_location` folder corresponding to a `stuno`) in a variable (potentially a list of dictionaries `[{}]`). This could optimize data retrieval and processing. (Implementation complexity: Hard, requires dictionary manipulation within lists).
*   **(Filename-Based Efficiency):** If filenames are consistently named, investigate filename-based optimizations to improve processing efficiency.
*   **(Data Merging Script):** Create a script to automatically merge location URLs from `whole.csv` with the rank data generated by `get_side_rank.py` for a more integrated dataset.

## Done

*   **Function `processing_list` Understanding:** Clarified the purpose and functionality of the `processing_list` function.
*   **Department Extraction:** Successfully extracted the department each student was admitted to.
*   **Dataframe Merging:** Implemented merging of dataframes column by column from different sources.
*   **正取、備取 Handling:** Addressed limitations in handling "正取" (Regular Admission) and "備取" (Backup Admission) ranks using `try`, `except`, and `else` blocks to improve robustness. Considered downloading test place lists and substring comparison for potential full test number extraction.
*   **Complete `<td` Count for University and Department:** Successfully extracted university and department information from `<td` count `i:9` and filtered for accepted university and department combinations.
*   **Department Name Sequence Fix:** Resolved issues with the department name sequence (`正1`, `正2` not correctly corresponding and being delayed by two positions).
*   **Location Link Extraction:** Implemented extraction of location links and suggested using Excel to filter and save relevant links.
*   **Student Number (`stuno`) Handling:**
    *   Discussed limitations in obtaining the full student number without accessing the test place link (currently using downloaded department lists).
    *   Extracted university and department applications and added them to the dataframe using a loop and `get_test_location_url.py` to generate `whole.csv`.
    *   Suggested using Excel to remove duplicate entries in `whole.csv`.
*   **`glob.iglob` Fix:** Resolved the issue of `glob.iglob` repeatedly finding the same `exam_location_folder`.
*   **Newline Character Fix:** Fixed the issue of newline characters `\n` appearing in student numbers within lists.
*   **Feature: Conditional Statement Addition:** Added `if` statements to improve script logic and control flow.
*   **Student-Specific Data Retrieval:** Implemented logic to allow each student in the list to find their own data using:
    1.  `stuno_second` and `stuno_last` (parts of student number).
    2.  Number comparison.
    3.  Department comparison (using list element identity check).
*   **Lower Number First Processing:** Implemented processing starting from lower numbers first using `glob.iglob(\`list\`)`.
*   **Code Refactoring:** Refactored Jupyter Notebook code into Python scripts (`get_location_url.py` and `get_side_rank.py`) for better code organization and script execution.

## WTF Section

*   **Excel for CSV Cleaning:** Acknowledged the use of Excel for cleaning and sorting CSV outputs, particularly for using Excel's cell fill-down feature to apply rules and checking/sorting results.
*   **`output_list_&_num.txt` Blank Lines:** Explained that blank lines in `output_list_&_num.txt` were due to starting the process from black table rows instead of white rows.
*   **`output_list_&_num_fixed.txt` Complete Exam Location List:** Clarified that `output_list_&_num_fixed.txt` addresses the blank line issue by separately finding complete exam location lists for white and dark table rows.