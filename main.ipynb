{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770d821a",
   "metadata": {},
   "source": [
    "# Requirment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60986cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (23.2.1)\n"
     ]
    }
   ],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2588d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from pytesseract) (23.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from pytesseract) (10.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (10.0.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (from opencv-python) (1.24.3)\n",
      "Requirement already satisfied: pathlib in c:\\users\\user\\desktop\\backup\\brt\\projects\\crawing-list-of-admitted-schools-for-students\\venv\\lib\\site-packages (1.0.1)\n",
      "Collecting ast\n",
      "  Downloading AST-0.0.2.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-cmvjdell\\ast_af27a5a016f54dbf817e7d342a0d55fb\\setup.py\", line 6, in <module>\n",
      "          README = codecs.open(os.path.join(here, 'AST/README'), encoding='utf8').read()\n",
      "        File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\codecs.py\", line 905, in open\n",
      "          file = builtins.open(filename, mode, buffering)\n",
      "      FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pip-install-cmvjdell\\\\ast_af27a5a016f54dbf817e7d342a0d55fb\\\\AST/README'\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "# !pip install tensorflow\n",
    "!pip install Pillow\n",
    "!pip install lxml\n",
    "!pip install opencv-python\n",
    "!pip install pathlib\n",
    "!pip install ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418b613",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f617ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "from pytesseract import pytesseract\n",
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157329d3",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd C:\\Users\\User\\Desktop\\backup\\brt\\Projects\\Crawing-List-of-Admitted-Schools-for-Students\\data\\\n",
    "\n",
    "for file in glob.iglob('department\\*交叉查榜*.html'):\n",
    "    # print(file)\n",
    "    processing_department(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fa0a1",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68113375",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processing_department(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        html = file.read()\n",
    "        # print(html)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    fields = ['rank', 'exam_location', 'stuno', 'univ',  'year', 'department_name', 'select']\n",
    "    df = pd.DataFrame(columns=fields)\n",
    "    df1 = pd.DataFrame(columns=fields)\n",
    "    df2 = pd.DataFrame(columns=fields)\n",
    "    \n",
    "    department_name = []\n",
    "    select = []\n",
    "\n",
    "    str = soup.head.find_all('title')[0].text\n",
    "    pos = str.find('年')\n",
    "    year = str[pos-3:pos]  # str[5:8] str[5], str[6], str[7]\n",
    "\n",
    "    str = soup.head.find_all('title')[0].text\n",
    "    pos = str.find('-')\n",
    "    univ = str[:pos].rstrip()\n",
    "\n",
    "    dark_list = soup.body.find_all(bgcolor=\"#DEDEDC\") # 褐色\n",
    "    df1 = processing_list(univ, year, dark_list, df, department_name, select)\n",
    "    \n",
    "    white_list = soup.body.find_all(bgcolor=\"#FFFFFF\") # 白色\n",
    "    df2 = processing_list(univ, year, white_list, df, department_name, select)\n",
    "    \n",
    "    # make two df cross-interleave\n",
    "    df = cross_interleave(df1, df2)\n",
    "\n",
    "    # explode list into df\n",
    "    df = df.explode(['department_name', 'select']).reset_index(drop=True)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # save to csv\n",
    "    result_dir = Path(r'C:\\Users\\User\\Desktop\\backup\\brt\\Projects\\Crawing-List-of-Admitted-Schools-for-Students\\result')\n",
    "    df.to_csv(path_or_buf = result_dir / f'{univ}.csv', encoding='utf-8-sig', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207f841",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c95da709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_list(univ, year, color_list, df, department_name, select):\n",
    "    print(univ)\n",
    "    for item in color_list:\n",
    "        color_list_item_scope1 = item.children # get message under the soup.body.find_all(bgcolor=\"\")\n",
    "        count_item = 0\n",
    "        exam_location = \"\"\n",
    "        for item_scope2 in color_list_item_scope1:\n",
    "            # print(\"i:\", item_scope2,\" \", count_item)\n",
    "            \"\"\"\n",
    "            count_item:3 = 正備取\n",
    "            count_item:5 = number\n",
    "            count_item:9 = 錄取校系\n",
    "            \"\"\"\n",
    "            if count_item == 3:\n",
    "                rank = img_detect(item_scope2 = item_scope2, img = r'rank.png', langage='chi_sim', oem_psm = '--oem 0 --psm 7')\n",
    "\n",
    "                    \n",
    "            elif count_item == 5:\n",
    "                stuno = img_detect(item_scope2 = item_scope2, img = r'num.png', langage='eng', oem_psm = '--oem 1 --psm 7')\n",
    "                \n",
    "                # get exam location\n",
    "                x = item.find_all('a')\n",
    "                exam_location = x[0].text[6:-1]\n",
    "                \n",
    "            elif count_item == 9:\n",
    "                tr_scope3 = item_scope2.find_all('tr')\n",
    "                department_name = []\n",
    "                select = []\n",
    "                for item_scope4 in tr_scope3:\n",
    "                    # print(\"item_scope4:\", count, rank, item_scope4)\n",
    "                    count_item_scope5 = 0\n",
    "                    for item_scope5 in item_scope4:\n",
    "                        # print('item_scope5:', count_, item_scope5)\n",
    "                        \"\"\"_summary_\n",
    "                        count_item_scope5: 1 徽章\n",
    "                        count_item_scope5: 3 大學\n",
    "                        count_item_scope5: 5 校系排名\n",
    "                        \"\"\"\n",
    "                        if count_item_scope5 == 1:\n",
    "                            img_tag = item_scope5.find('img')\n",
    "                            # print(img_tag)\n",
    "                            if img_tag is not None:\n",
    "                                img_tag_ = True\n",
    "                            else:\n",
    "                                img_tag_ = False\n",
    "                            select.append(img_tag_)\n",
    "                            \n",
    "                        elif count_item_scope5 == 3:\n",
    "                            # We'll drop later to suit up with img_tag_\n",
    "                            if item_scope5.text != \"\":\n",
    "                                department_name.append(item_scope5.text)\n",
    "                            else:\n",
    "                                department_name.append(np.nan)\n",
    "                            \n",
    "                        elif count_item_scope5 == 5:\n",
    "                            # to get the rank of each dept\n",
    "                            pass\n",
    "                        \n",
    "                        count_item_scope5 += 1\n",
    "\n",
    "            count_item += 1\n",
    "        \n",
    "        dict = {'rank': rank, 'exam_location': exam_location, 'stuno': stuno, 'univ': univ, 'year': year, 'department_name': department_name, 'select': select}\n",
    "        df = pd.concat([df, pd.DataFrame([dict])], ignore_index = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cross_interleave(df1, df2):\n",
    "    # make two df cross-interleave\n",
    "    df = pd.DataFrame()\n",
    "    max_len = max(len(df1), len(df2))\n",
    "    for line in range(max_len):\n",
    "        if line < len(df1):\n",
    "            df = pd.concat([df, df1.iloc[[line]]], ignore_index=True)\n",
    "        if line < len(df2):\n",
    "            df = pd.concat([df, df2.iloc[[line]]], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32d4cd",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad087c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_detect(item_scope2, img, langage, oem_psm):\n",
    "    try:\n",
    "        base64_str = item_scope2.find_all('img')[0]['src'].split(',')[1]\n",
    "    except IndexError:\n",
    "        return \"None\"\n",
    "    else:\n",
    "        with open(img, \"wb\") as fh:\n",
    "            fh.write(base64.decodebytes(bytes(base64_str, 'utf-8')))\n",
    "        path_to_tesseract = Path(r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe')\n",
    "        pytesseract.tesseract_cmd = path_to_tesseract\n",
    "        path_to_image = img\n",
    "        thresholding_(path_to_image)\n",
    "        return pytesseract.image_to_string(Image.open(path_to_image), lang=langage, config=oem_psm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f4541",
   "metadata": {},
   "source": [
    "##### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fd1384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding_(png):\n",
    "    if png[0:3] == \"num\":\n",
    "        image = cv2.imread(png)\n",
    "        gray = get_grayscale(image)\n",
    "        thresh_ = thresholding(gray)\n",
    "        cv2.imwrite('num.png', thresh_)\n",
    "    elif png[0:4] == \"rank\":\n",
    "        image = cv2.imread(png)\n",
    "        gray = get_grayscale(image)\n",
    "        thresh_ = thresholding(gray)\n",
    "        cv2.imwrite('rank.png', thresh_)        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54cf94",
   "metadata": {},
   "source": [
    "###### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fe78ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to grayscale\n",
    "def get_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Remove noise from the image\n",
    "def remove_noise(image):\n",
    "    return cv2.medianBlur(image, 5)\n",
    " \n",
    "# Thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# Dilation\n",
    "def dilate(image):\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    return cv2.dilate(image, kernel, iterations=1)\n",
    "    \n",
    "# Erosion\n",
    "def erode(image):\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations=1)\n",
    "\n",
    "# Opening - erosion followed by dilation\n",
    "def opening(image):\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "# Canny edge detection\n",
    "def canny(image):\n",
    "    return cv2.Canny(image, 100, 200)\n",
    "\n",
    "# Skew correction\n",
    "def deskew(image):\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "# Template matching\n",
    "def match_template(image, template):\n",
    "    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822792cf",
   "metadata": {},
   "source": [
    "# Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a31a27a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_location\\國立嘉義女中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立嘉義高中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立宜蘭大學學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立家齊高中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立新竹女中學測查榜- 112年大學 _ 科大交叉查榜-1www.com.tw.html\n",
      "test_location\\國立新竹女中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立臺南女中學測查榜- 112年大學 _ 科大交叉查榜-1www.com.tw.html\n",
      "test_location\\國立臺南女中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立臺灣師大附中學測查榜- 112年大學 _ 科大交叉查榜-1www.com.tw.html\n",
      "test_location\\國立臺灣師大附中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n",
      "test_location\\國立華僑高中學測查榜- 112年大學 _ 科大交叉查榜-1-www.com.tw.html\n",
      "test_location\\國立華僑高中學測查榜- 112年大學 _ 科大交叉查榜-2www.com.tw.html\n",
      "test_location\\國立華僑高中學測查榜- 112年大學 _ 科大交叉查榜-www.com.tw.html\n"
     ]
    }
   ],
   "source": [
    "scho = '國立'\n",
    "for file in glob.iglob(f'test_location/{scho}*學測查榜*.html'):\n",
    "    processing_numbers(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b99e8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_numbers(filename):\n",
    "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
    "        html = file.read()\n",
    "        # print(html)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "    fields = ['rank', 'exam_location', 'stuno', 'univ',  'year', 'department_name', 'select']\n",
    "    df = pd.DataFrame(columns=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589366eb",
   "metadata": {},
   "source": [
    "### data_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06729b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111****00\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "str1 = 'lL1x%\\\"«oO'\n",
    "\n",
    "char_to_replace = {'l':'1', 'L':'1', 'x':'*', '%':'*', '\\\"':'*', '«':'*', 'o':'0', 'O':'0'}\n",
    "\n",
    "# Iterate over all key-value pairs in dictionary \n",
    "for key, value in char_to_replace.items():\n",
    "    # Replace key character with value character in string\n",
    "    str1 = str1.replace(key, value)     \n",
    "\n",
    "print(str1)\n",
    "print(type(str1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
